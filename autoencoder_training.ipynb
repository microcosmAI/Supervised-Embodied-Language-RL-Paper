{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "from autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, image_size=(64, 64, 3)):\n",
    "    image_names = os.listdir(data_dir)\n",
    "    image_names = [x for x in image_names if x.endswith('.jpg') or x.endswith('.png') or x.endswith('.jpeg')]\n",
    "    train_data = []\n",
    "    for image_name in image_names:\n",
    "        image = cv2.imread(os.path.join(data_dir, image_name))\n",
    "        image = image / 255.0\n",
    "        train_data.append(image)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dssim_loss(y_true, y_pred):\n",
    "    return 1/2 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))/2\n",
    "\n",
    "def model_name(dataset_name, latent_dim, training_loss, batch_size):\n",
    "    return f'{dataset_name}_dim_{latent_dim}_loss_{training_loss}_batch_{batch_size}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_shape=(64, 64, 3), latent_dim=50, training_loss='ssim', batch_size=8, epochs=20):\n",
    "\n",
    "    # Load data\n",
    "    X_train = load_data(\"images/\", input_shape)\n",
    "    X_train = np.array(X_train)\n",
    "\n",
    "    Y_train = X_train\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, shuffle=True)\n",
    "    print(f'Loaded data: train {X_train.shape}, validation {Y_val.shape}')\n",
    "\n",
    "    # Create autoencoder\n",
    "    autoencoder = Autoencoder(input_shape=input_shape, latent_dim=latent_dim)\n",
    "\n",
    "    # Set training loss function and optimizer\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "\n",
    "    opt = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    if training_loss == 'mse':\n",
    "        autoencoder.compile(loss='mse', optimizer=opt)\n",
    "    elif training_loss == 'ssim':\n",
    "        autoencoder.compile(loss=dssim_loss, optimizer=opt)\n",
    "\n",
    "    # Training\n",
    "    autoencoder.fit(\n",
    "      x=X_train, y=Y_train,\n",
    "      epochs=epochs, batch_size=batch_size,\n",
    "      shuffle=True, validation_data=(X_val, Y_val),\n",
    "      callbacks=[])\n",
    "    \n",
    "    return autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'grid'\n",
    "latent_dim = 30\n",
    "batch_size = 8\n",
    "training_loss = 'ssim'\n",
    "\n",
    "autoencoder = train_model( \n",
    "    input_shape=(64, 64, 3),\n",
    "    latent_dim = latent_dim,\n",
    "    training_loss = training_loss,\n",
    "    batch_size = batch_size,\n",
    "    epochs = 80)\n",
    "\n",
    "autoencoder.encoder.save_weights(\"models/encoder30.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
